## Brainstorming Ideas

#### Topology
* T1. [**IN PROCESS**] Custom skeleton (e.g. model 1 individual finger, or a hand with 6 fingers)
* T2. [**IN PROCESS**] Custom shapes (e.g. different 3D shapes of body parts)
* T3. [**IN PROCESS**] Custom degrees of freedom in joints
* T4. [**IN PROCESS**] Custom ranges of joint rotations
* T5. Inverse kinematics
* T6. Soft tissue and deformations

#### Sensors and feedback
* S1. Custom positions of sensors
* S2. Feedback from joint positions
* S3. Collision detection with external objects
* S4. Self-collision detection
* S5. Visual feedback with color heatmaps
* S6. Visual feedback with vectors
* S7. Visual feedback with tables/graphs
* S8. Gravity for gripped objects

#### Interface
* I1. Interface API for using from outside
* I2. [**IN PROCESS**] Programming control via API
* I3. [**IN PROCESS**] API for model definition
* I4. [**IN PROCESS**] API for model motion

#### Visual
* V1. Interactive controls
* V2. Stock objects to grap and hold
* V3. Importing GLTF body parts
* V4. Exporting GLTF models
* V5. Customs warning (aka virtual pain)

#### Postures
* P1. Mapping between input data and posture
* P2. Predefined collection of postures
* P3. Predefined animations
* P4. [**IN PROCESS**] Scenes with several models
* P5. Self-balancing of pressure
* P6. [**IN PROCESS**]Predefined models (e.g. hands)
* P7. Self-learning mode
* P8. Macros mode (e.g. grip for rod, ball, cup; handshake, ...)

#### Documents
* D1. [**IN PROCESS**] GitHub project
* D2. [**IN PROCESS**] User documentation
* D3. Educational content
* D4. Academic papers
* D5. Social network disseminations (e.g. videos)

#### Initially needed information
* Close-up images of hand prototype
* Details about input signals and communication
* Mapping between input signals and angles
* Mapping between collision and feedback signals